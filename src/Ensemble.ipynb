{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd # not key to functionality of kernel\n",
    "import glob\n",
    "import timeit\n",
    "import sys\n",
    "import csv\n",
    "import string\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import mean_average_precision_calculator as map_calculator\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.observer import JSONLogger\n",
    "from bayes_opt.util import load_logs\n",
    "from bayes_opt.event import Events\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Eval mAP@N metric from inference file.\"\"\"\n",
    "\n",
    "class Labels(object):\n",
    "  \"\"\"Contains the class to hold label objects.\n",
    "\n",
    "  This class can serialize and de-serialize the groundtruths.\n",
    "  The ground truth is in a mapping from (segment_id, class_id) -> label_score.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, labels):\n",
    "    \"\"\"__init__ method.\"\"\"\n",
    "    self._labels = labels\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    \"\"\"Return the ground truth mapping. See class docstring for details.\"\"\"\n",
    "    return self._labels\n",
    "\n",
    "  def to_file(self, file_name):\n",
    "    \"\"\"Materialize the GT mapping to file.\"\"\"\n",
    "    with tf.gfile.Open(file_name, \"w\") as fobj:\n",
    "      for k, v in self._labels.items():\n",
    "        seg_id, label = k\n",
    "        line = \"%s,%s,%s\\n\" % (seg_id, label, v)\n",
    "        fobj.write(line)\n",
    "\n",
    "  @classmethod\n",
    "  def from_file(cls, file_name):\n",
    "    \"\"\"Read the GT mapping from cached file.\"\"\"\n",
    "    labels = {}\n",
    "    with tf.gfile.Open(file_name) as fobj:\n",
    "      for line in fobj:\n",
    "        line = line.strip().strip(\"\\n\")\n",
    "        seg_id, label, score = line.split(\",\")\n",
    "        labels[(seg_id, int(label))] = float(score)\n",
    "    return cls(labels)\n",
    "\n",
    "\n",
    "def read_labels(data_pattern, cache_path=\"\"):\n",
    "  \"\"\"Read labels from TFRecords.\n",
    "\n",
    "  Args:\n",
    "    data_pattern: the data pattern to the TFRecords.\n",
    "    cache_path: the cache path for the label file.\n",
    "\n",
    "  Returns:\n",
    "    a Labels object.\n",
    "  \"\"\"\n",
    "  if cache_path:\n",
    "    if tf.gfile.Exists(cache_path):\n",
    "      tf.logging.info(\"Reading cached labels from %s...\" % cache_path)\n",
    "      return Labels.from_file(cache_path)\n",
    "  tf.enable_eager_execution()\n",
    "\n",
    "  if 'validate' in data_pattern:\n",
    "    with tf.name_scope(\"eval_input\"):\n",
    "      '''\n",
    "      # randomly chosen 60 validate files\n",
    "      # note that validate file names are different on gcloud and locally, due to `curl` download command\n",
    "      results = []\n",
    "      for i in range(3844):\n",
    "          results.append(str(i).zfill(4))\n",
    "      random.seed(7)\n",
    "      random.shuffle(results)\n",
    "      validate_file_nums = results[:300]\n",
    "\n",
    "      validate_file_list_60 = [data_pattern.split('*')[0]\\\n",
    "                               + x +'.tfrecord' for x in validate_file_nums]\n",
    "      data_paths = validate_file_list_60\n",
    "      #print(data_paths)\n",
    "      '''\n",
    "      a_list = list(string.ascii_lowercase)\n",
    "      A_list = list(string.ascii_uppercase)\n",
    "\n",
    "      n_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "      n_list = n_list + a_list + A_list\n",
    "\n",
    "      results = []\n",
    "      for a in n_list:\n",
    "        for n in n_list:\n",
    "          results.append(n + a)\n",
    "      #random.seed(7)\n",
    "      #random.shuffle(results)\n",
    "      validate_file_nums = results[:300]\n",
    "      validate_file_list_60 = [data_pattern.split('*')[0]\\\n",
    "                               + x +'.tfrecord' for x in validate_file_nums]\n",
    "      data_paths = validate_file_list_60\n",
    "      #print(data_paths)\n",
    "  else:\n",
    "    data_paths = tf.gfile.Glob(data_pattern)\n",
    "    print(data_paths)\n",
    "\n",
    "  ds = tf.data.TFRecordDataset(data_paths, num_parallel_reads=50)\n",
    "  context_features = {\n",
    "      \"id\": tf.FixedLenFeature([], tf.string),\n",
    "      \"segment_labels\": tf.VarLenFeature(tf.int64),\n",
    "      \"segment_start_times\": tf.VarLenFeature(tf.int64),\n",
    "      \"segment_scores\": tf.VarLenFeature(tf.float32)\n",
    "  }\n",
    "\n",
    "  def _parse_se_func(sequence_example):\n",
    "    return tf.parse_single_sequence_example(\n",
    "        sequence_example, context_features=context_features)\n",
    "\n",
    "  ds = ds.map(_parse_se_func)\n",
    "  rated_labels = {}\n",
    "  tf.logging.info(\"Reading labels from TFRecords...\")\n",
    "  last_batch = 0\n",
    "  batch_size = 5000\n",
    "  for cxt_feature_val, _ in ds:\n",
    "    video_id = cxt_feature_val[\"id\"].numpy()\n",
    "    segment_labels = cxt_feature_val[\"segment_labels\"].values.numpy()\n",
    "    segment_start_times = cxt_feature_val[\"segment_start_times\"].values.numpy()\n",
    "    segment_scores = cxt_feature_val[\"segment_scores\"].values.numpy()\n",
    "    for label, start_time, score in zip(segment_labels, segment_start_times,\n",
    "                                        segment_scores):\n",
    "      rated_labels[(\"%s:%d\" % (video_id.decode(\"utf-8\"), start_time), label)] = score\n",
    "    batch_id = len(rated_labels) // batch_size\n",
    "    if batch_id != last_batch:\n",
    "      tf.logging.info(\"%d examples processed.\", len(rated_labels))\n",
    "      last_batch = batch_id\n",
    "  tf.logging.info(\"Finish reading labels from TFRecords...\")\n",
    "  labels_obj = Labels(rated_labels)\n",
    "  if cache_path:\n",
    "    tf.logging.info(\"Caching labels to %s...\" % cache_path)\n",
    "    labels_obj.to_file(cache_path)\n",
    "  return labels_obj\n",
    "\n",
    "\n",
    "def read_segment_predictions(file_path, labels, top_n=None):\n",
    "  \"\"\"Read segement predictions.\n",
    "\n",
    "  Args:\n",
    "    file_path: the submission file path.\n",
    "    labels: a Labels object containing the eval labels.\n",
    "    top_n: the per-class class capping.\n",
    "\n",
    "  Returns:\n",
    "    a segment prediction list for each classes.\n",
    "  \"\"\"\n",
    "  cls_preds = {}  # A label_id to pred list mapping.\n",
    "  with tf.gfile.Open(file_path) as fobj:\n",
    "    tf.logging.info(\"Reading predictions from %s...\" % file_path)\n",
    "    for line in fobj:\n",
    "      label_id, pred_ids_val = line.split(\",\")\n",
    "      pred_ids = pred_ids_val.split(\" \")\n",
    "      if top_n:\n",
    "        pred_ids = pred_ids[:top_n]\n",
    "      pred_ids = [\n",
    "          pred_id for pred_id in pred_ids\n",
    "          if (pred_id, int(label_id)) in labels.labels\n",
    "      ]\n",
    "      cls_preds[int(label_id)] = pred_ids\n",
    "      if len(cls_preds) % 50 == 0:\n",
    "        tf.logging.info(\"Processed %d classes...\" % len(cls_preds))\n",
    "    tf.logging.info(\"Finish reading predictions.\")\n",
    "  return cls_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_sort_files(sub_files):\n",
    "    for i, file in enumerate( sub_files ):\n",
    "        print(file)\n",
    "        sample_sub = pd.read_csv('./sample_submission.csv')\n",
    "        column_names = sample_sub.columns\n",
    "        sample_sub.set_index('Class', inplace=True)\n",
    "\n",
    "        pred = pd.read_csv(file, names=column_names)\n",
    "        print(pred.shape)\n",
    "        print(pred.head())\n",
    "        pred.set_index('Class', inplace=True)\n",
    "        for cls in sample_sub.index:\n",
    "            if cls in pred.index:\n",
    "                sample_sub.loc[cls, 'Segments'] = pred.loc[cls, 'Segments']\n",
    "        sample_sub.reset_index().to_csv(SUBDIR_SINGLE_MODEL+'sorted/'+file.split('\\\\')[-1].split('.')[0]+'_sorted.csv', index=None)\n",
    "\n",
    "def combine_tta(files):\n",
    "    for file in files:\n",
    "        print(file)\n",
    "        sub_files = []\n",
    "        sub_files.append(SUBDIR_SINGLE_MODEL + file + '_oof_sorted.csv')\n",
    "        sub_files.append(SUBDIR_SINGLE_MODEL + file + '_oof_shift1_sorted.csv')\n",
    "        sub_files.append(SUBDIR_SINGLE_MODEL + file + '_oof_shift-1_sorted.csv')\n",
    "\n",
    "        lg = len(sub_files)\n",
    "        sub = []\n",
    "\n",
    "        w0 = 3.72384689385059\n",
    "        sub_weight = []\n",
    "        w_sum = 0\n",
    "        count = 0\n",
    "        for i, sub_file in enumerate( sub_files ):\n",
    "            try:\n",
    "                print(\"Reading {}: {}\". format(i, sub_file))\n",
    "                reader = csv.DictReader(open(sub_file,\"r\"))\n",
    "                sub.append(sorted(reader, key=lambda d: str(d[Hlabel])))\n",
    "            except:\n",
    "                print(\"Can't find  it\")\n",
    "                lg -= 1\n",
    "                continue\n",
    "            sub_weight.append(1)\n",
    "            w_sum += 1\n",
    "            count += 1\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        print('Start submission generation')\n",
    "\n",
    "        place_weights = {}\n",
    "        for i in range(npt):\n",
    "            place_weights[i] = ( w0 / (i + w0) )\n",
    "\n",
    "        print(f'W0: {w0}; Weight Percentage: {[i/w_sum*100 for i in sub_weight]}')\n",
    "\n",
    "        result = {}\n",
    "        result['Class'] = []\n",
    "        result['Segments'] = []\n",
    "        for p, row in enumerate(sub[0]):\n",
    "            target_weight = {}\n",
    "            for s in range(lg):\n",
    "                row1 = sub[s][p]\n",
    "                for ind, trgt in enumerate(row1[Htarget].split(' ')):\n",
    "                    target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])\n",
    "            tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]\n",
    "            result['Class'].append(row1[Hlabel])\n",
    "            result['Segments'].append(\" \".join(tops_trgt))\n",
    "\n",
    "        result_df = pd.DataFrame.from_dict(result)\n",
    "        print(result_df.shape)\n",
    "\n",
    "        print(f'Finished submission oof generation! Spent {timeit.default_timer() - start}s')\n",
    "\n",
    "        result_df.to_csv(SUBDIR_SINGLE_MODEL + f'sorted_single/{file}_all{count}.csv', index=False)\n",
    "        \n",
    "def submission_generate(sub, w1, w2, w3, w4, w5, w6, w7, w8, w9):\n",
    "    start = timeit.default_timer()\n",
    "    print('Start submission generation')\n",
    "\n",
    "    place_weights = {}\n",
    "    for i in range(npt):\n",
    "        place_weights[i] = ( w0 / (i + w0) )\n",
    "\n",
    "    sub_weight = [w1, w2, w3, w4, w5, w6, w7, w8, w9]\n",
    "    w_sum = w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9\n",
    "\n",
    "    print(f'W0: {w0}; Weight Percentage: {[i/w_sum*100 for i in sub_weight]}')\n",
    "\n",
    "    result = {}\n",
    "    result['Class'] = []\n",
    "    result['Segments'] = []\n",
    "    for p, row in enumerate(sub[0]):\n",
    "        target_weight = {}\n",
    "        for s in range(lg):\n",
    "            row1 = sub[s][p]\n",
    "            for ind, trgt in enumerate(row1[Htarget].split(' ')):\n",
    "                target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])\n",
    "        tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]\n",
    "        result['Class'].append(row1[Hlabel])\n",
    "        result['Segments'].append(\" \".join(tops_trgt))\n",
    "\n",
    "    result_df = pd.DataFrame.from_dict(result)\n",
    "    \n",
    "    print(f'Finished submission oof generation! Spent {timeit.default_timer() - start}s')\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Bayesian Optimization for OOF Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Prepare Sorted OOF_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the fold that contains your oof files\n",
    "SUBDIR_SINGLE_MODEL = './oof/'\n",
    "sub_files = glob.glob(SUBDIR_SINGLE_MODEL+'*.csv')\n",
    "\n",
    "# Sort files based on class id for future ensemble use\n",
    "prep_sort_files(sub_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Combine Different TTA Inferece Files Into One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SUBDIR_SINGLE_MODEL = './oof/sorted/'\n",
    "sub_files = glob.glob(SUBDIR_SINGLE_MODEL+'*.csv')\n",
    "\n",
    "temp = []\n",
    "for file in sub_files:\n",
    "    temp.append(file.split('_oof')[0].split('\\\\')[1])\n",
    "temp = set(temp)\n",
    "files = []\n",
    "for file in temp:\n",
    "    files.append(file)\n",
    "    \n",
    "Hlabel = 'Class' \n",
    "Htarget = 'Segments'\n",
    "npt = 100000 # number of places in target\n",
    "\n",
    "combine_tta(files)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data for bayesian optimization\n",
    "SUBDIR_SINGLE_MODEL = './oof/sorted/sorted_single/'\n",
    "sub_files = glob.glob(SUBDIR_SINGLE_MODEL+'*.csv')\n",
    "\n",
    "lg = len(sub_files)\n",
    "sub = [None]*lg\n",
    "for i, file in enumerate( sub_files ):\n",
    "    ## input files ##\n",
    "    print(\"Reading {}: {}\". format(i, file))\n",
    "    reader = csv.DictReader(open(file,\"r\"))\n",
    "    sub[i] = sorted(reader, key=lambda d: str(d[Hlabel]))\n",
    "    #if i > 1:\n",
    "    #    break\n",
    "\n",
    "# Use the same valid data pattern used for training. If train on the google cloud, you need to download them manually\n",
    "valid_data_pattern = './inputs/data/frame/3/validate_data/validate*.tfrecord'\n",
    "\n",
    "eval_labels = read_labels(valid_data_pattern)\n",
    "positive_counter = {}\n",
    "for k, v in eval_labels.labels.items():\n",
    "    _, label_id = k\n",
    "    if v > 0:\n",
    "        positive_counter[label_id] = positive_counter.get(label_id, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Run Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble_bayesian(w0, w1, w2, w3, w4, w5, w6, w7, w8, w9):\n",
    "    start = timeit.default_timer()\n",
    "    print('Start submission generation')\n",
    "    \n",
    "    place_weights = {}\n",
    "    for i in range(npt):\n",
    "        place_weights[i] = ( w0 / (i + w0) )\n",
    "    \n",
    "    sub_weight = [w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9]\n",
    "    w_sum = w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9\n",
    "    print(f'W0: {w0}; Weight Percentage: {sub_weight / w_sum}')\n",
    "    \n",
    "    result = {}\n",
    "    ## output file ##\n",
    "    for p, row in enumerate(sub[0]):\n",
    "        target_weight = {}\n",
    "        for s in range(lg):\n",
    "            row1 = sub[s][p]\n",
    "            for ind, trgt in enumerate(row1[Htarget].split(' ')):\n",
    "                target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])\n",
    "        tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]\n",
    "        result[row1[Hlabel]] = tops_trgt\n",
    "    \n",
    "    print(f'Finished submission generation! Spent {timeit.default_timer() - start}s')\n",
    "    \n",
    "    start = timeit.default_timer()\n",
    "    print('Start evaluation')\n",
    "    \n",
    "    seg_preds = {}\n",
    "    for label_id in result.keys():\n",
    "        pred_ids = result[label_id][:100000]\n",
    "        pred_ids = [\n",
    "          pred_id for pred_id in pred_ids\n",
    "          if (pred_id, int(label_id)) in eval_labels.labels\n",
    "        ]\n",
    "        seg_preds[int(label_id)] = pred_ids\n",
    "    #\"\"\"\n",
    "    map_cal = map_calculator.MeanAveragePrecisionCalculator(len(seg_preds))\n",
    "    seg_labels = []\n",
    "    seg_scored_preds = []\n",
    "    num_positives = []\n",
    "    for label_id in sorted(seg_preds):\n",
    "        class_preds = seg_preds[label_id]\n",
    "        seg_label = [eval_labels.labels[(pred, label_id)] for pred in class_preds]\n",
    "        seg_labels.append(seg_label)\n",
    "        seg_scored_pred = []\n",
    "        if class_preds:\n",
    "            seg_scored_pred = [\n",
    "              float(x) / len(class_preds) for x in range(len(class_preds), 0, -1)\n",
    "            ]\n",
    "        seg_scored_preds.append(seg_scored_pred)\n",
    "        num_positives.append(positive_counter.get(label_id, 0))\n",
    "    map_cal.accumulate(seg_scored_preds, seg_labels, num_positives)\n",
    "    map_at_n = np.mean(map_cal.peek_map_at_n())\n",
    "    print(f'Finished evaluation with MAP {map_at_n}! Spent {timeit.default_timer() - start}s')\n",
    "    print('')    \n",
    "    \n",
    "    return map_at_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the range of potential space of each parameters\n",
    "optimizer = BayesianOptimization(\n",
    "    f=run_ensemble_bayesian,\n",
    "    pbounds={\n",
    "        \"w0\": [0, 10], \n",
    "        \"w1\": [0, 200], \n",
    "        \"w2\": [0, 200], \n",
    "        \"w3\": [0, 200], \n",
    "        \"w4\": [0, 200], \n",
    "        \"w5\": [0, 200], \n",
    "        \"w6\": [0, 200], \n",
    "        \"w7\": [0, 200], \n",
    "        \"w8\": [0, 200],\n",
    "        \"w9\": [0, 200],\n",
    "    },\n",
    "    verbose=2,\n",
    "    random_state=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uncomment to load previous logs and continue tuning\n",
    "#from bayes_opt.util import load_logs\n",
    "#load_logs(optimizer, logs=[\"./logs.json\"])\n",
    "#print(\"New optimizer is now aware of {} points.\".format(len(optimizer.space)))\n",
    "\n",
    "logger = JSONLogger(path=\"./logs.json\")\n",
    "optimizer.subscribe(Events.OPTMIZATION_STEP, logger)\n",
    "optimizer.maximize(\n",
    "    init_points=3,\n",
    "    n_iter=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Generate Final Ensembled OOF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimizer.max)\n",
    "params = optimizer.max['params']\n",
    "\n",
    "w0 = params['w0']\n",
    "w1 = params['w1']\n",
    "w2 = params['w2']\n",
    "w3 = params['w3']\n",
    "w4 = params['w4']\n",
    "w5 = params['w5']\n",
    "w6 = params['w6']\n",
    "w7 = params['w7']\n",
    "w8 = params['w8']\n",
    "w9 = params['w9']\n",
    "\n",
    "result_df = submission_generate(sub, w1, w2, w3, w4, w5, w6, w7, w8, w9)\n",
    "print(result_df.shape)\n",
    "\n",
    "result_df.to_csv(SUBDIR_SINGLE_MODEL + '/blend/blend1_9_oof.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Final Submission File Genration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Prepare Sorted Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the fold that contains your oof files\n",
    "SUBDIR_SINGLE_MODEL = './submission/'\n",
    "sub_files = glob.glob(SUBDIR_SINGLE_MODEL+'*.csv')\n",
    "\n",
    "# Sort files based on class id for future ensemble use\n",
    "prep_sort_files(sub_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Combine Different TTA Inferece Files Into One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBDIR_SINGLE_MODEL = './submission/sorted/'\n",
    "sub_files = glob.glob(SUBDIR_SINGLE_MODEL+'*.csv')\n",
    "\n",
    "temp = []\n",
    "for file in sub_files:\n",
    "    temp.append(file.split('_finetune')[0].split('\\\\')[1])\n",
    "temp = set(temp)\n",
    "files = []\n",
    "for file in temp:\n",
    "    files.append(file)\n",
    "    \n",
    "Hlabel = 'Class' \n",
    "Htarget = 'Segments'\n",
    "npt = 100000 # number of places in target\n",
    "\n",
    "combine_tta(files)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Generate Final Ensembled Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single\\BertCross_bhidden2_bhead8_milattn_predictions_all3.csv\n",
      "Reading 1: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single\\BertCross_bhidden2_bhead8_milmean_predictions_all3.csv\n",
      "Reading 2: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single\\Bert_bhidden2_bhead12_milfirst_predictions_all4.csv\n",
      "Reading 3: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single\\Bert_bhidden2_bhead12_milmean_predictions_all4.csv\n",
      "Reading 4: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single\\Bert_bhidden2_bhead12_predictions_all3.csv\n",
      "Reading 5: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single\\Bert_bhidden3_bhead12_predictions_all3.csv\n",
      "Reading 6: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single\\MixNeXtVladModel_iter300_predictions_all3.csv\n",
      "Reading 7: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single\\MixNeXtVladModel_iter60_predictions_shift21.csv\n",
      "Reading 8: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single\\NetVLADModelLF_hs1024_cs8_vlad_predictions_all3.csv\n"
     ]
    }
   ],
   "source": [
    "# Read data for bayesian optimization\n",
    "SUBDIR_SINGLE_MODEL = './submission/sorted/sorted_single/'\n",
    "SUBDIR_SINGLE_MODEL = 'G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/'\n",
    "sub_files = glob.glob(SUBDIR_SINGLE_MODEL+'*.csv')\n",
    "\n",
    "Hlabel = 'Class' \n",
    "Htarget = 'Segments'\n",
    "npt = 100000 # number of places in target\n",
    "\n",
    "lg = len(sub_files)\n",
    "sub = [None]*lg\n",
    "for i, file in enumerate( sub_files ):\n",
    "    ## input files ##\n",
    "    print(\"Reading {}: {}\". format(i, file))\n",
    "    reader = csv.DictReader(open(file,\"r\"))\n",
    "    sub[i] = sorted(reader, key=lambda d: str(d[Hlabel]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w0': 6.058430980648939, 'w1': 30.31334691, 'w2': 24.89993526, 'w3': 99.90311048, 'w4': 78.51473969, 'w5': 14.28880107, 'w6': 70.49505912, 'w7': 101.7196348, 'w8': 87.66822585, 'w9': 1.108667853}\n",
      "Start submission generation\n",
      "W0: 6.058430980648939; Weight Percentage: [5.956506319304639, 4.892782778715159, 19.630742545819054, 15.427974499502199, 2.8077181355604353, 13.85212482061863, 19.98768559877112, 17.22661449519733, 0.2178508065114347]\n",
      "Finished submission oof generation! Spent 781.2326618s\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "#params = optimizer.max['params']\n",
    "params = {\n",
    "  'w0': 6.058430980648939,\n",
    "  'w1': 30.31334691,\n",
    "  'w2': 24.89993526,\n",
    "  'w3': 99.90311048,\n",
    "  'w4': 78.51473969,\n",
    "  'w5': 14.28880107,\n",
    "  'w6': 70.49505912,\n",
    "  'w7': 101.7196348,\n",
    "  'w8': 87.66822585,\n",
    "  'w9': 1.108667853\n",
    "}\n",
    "print(params)\n",
    "\n",
    "w0 = params['w0']\n",
    "w1 = params['w1']\n",
    "w2 = params['w2']\n",
    "w3 = params['w3']\n",
    "w4 = params['w4']\n",
    "w5 = params['w5']\n",
    "w6 = params['w6']\n",
    "w7 = params['w7']\n",
    "w8 = params['w8']\n",
    "w9 = params['w9']\n",
    "\n",
    "result_df = submission_generate(sub, w1, w2, w3, w4, w5, w6, w7, w8, w9)\n",
    "print(result_df.shape)\n",
    "\n",
    "result_df.to_csv(SUBDIR_SINGLE_MODEL + '/blend/blend_1-9_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start submission generation\n",
      "W0: 6.058430980648939; Weight Percentage: [11.862980731005676, 8.83132994770796, 17.24338852446555, 19.649857540823565, 6.744235019649597, 0.0, 15.44299714343935, 20.007148159062996, 0.2180629338453079]\n",
      "(1000, 2)\n",
      "   Class                                           Segments\n",
      "0    100  AmKH:155 iAON:285 ZvNy:140 ari1:160 yeEH:150 7...\n",
      "1   1000  pyZt:220 pyZt:180 pyZt:175 pyZt:170 pyZt:210 p...\n",
      "2   1001  Odh2:140 TLOW:150 3y2q:50 eCpP:0 1bhQ:45 1bhQ:...\n",
      "3   1002  rtuW:125 rtuW:130 rtuW:20 rtuW:120 rtuW:140 rt...\n",
      "4   1005  lSZ6:5 37Nx:140 37Nx:135 lSZ6:0 uDuW:105 lSZ6:...\n",
      "Finished submission generation! Spent 958.7436024000053s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "params = {\n",
    "  'w0': 6.058430980648939,\n",
    "  'w1': 14.2888010723968,\n",
    "  'w2': 99.90311047695229,\n",
    "  'w3': 70.49505912223611,\n",
    "  'w4': 78.51473969408502,\n",
    "  'w5': 54.899935258593764,\n",
    "  'w6': 47.668225850046696+40,\n",
    "  'w8': 1.1086678532452932\n",
    "}\n",
    "'''\n",
    "params = {\n",
    "  'w0': 6.058430980648939,\n",
    "  'w1': 30.31334691,\n",
    "  'w2': 24.89993526,\n",
    "  'w3': 99.90311048,\n",
    "  'w4': 78.51473969,\n",
    "  'w5': 14.28880107,\n",
    "  'w6': 70.49505912,\n",
    "  'w7': 101.7196348,\n",
    "  'w8': 87.66822585,\n",
    "  'w9': 1.108667853\n",
    "}\n",
    "#'''\n",
    "#v3\n",
    "params = {\n",
    "  'w0': 6.058430980648939,\n",
    "  'w1': 30.31334691,\n",
    "  'w2': 24.89993526,\n",
    "  'w3': 87.66822585,\n",
    "  'w4': 99.90311048,\n",
    "  'w5': 14.28880107,\n",
    "  'w6': 70.49505912,\n",
    "  'w7': 78.51473969,\n",
    "  'w8': 101.7196348,\n",
    "  'w9': 1.108667853\n",
    "}\n",
    "#'''\n",
    "\n",
    "params = {\n",
    "  'w0': 6.058430980648939,\n",
    "  'w1': 60.31334691,\n",
    "  'w2': 44.89993526,\n",
    "  'w3': 87.66822585,\n",
    "  'w4': 99.90311048,\n",
    "  'w5': 34.28880107,\n",
    "  'w6': 0,\n",
    "  'w7': 78.51473969,\n",
    "  'w8': 101.7196348,\n",
    "  'w9': 1.108667853\n",
    "}\n",
    "\n",
    "w0 = params['w0']\n",
    "w1 = params['w1']\n",
    "w2 = params['w2']\n",
    "w3 = params['w3']\n",
    "w4 = params['w4']\n",
    "w5 = params['w5']\n",
    "w6 = params['w6']\n",
    "w7 = params['w7']\n",
    "w8 = params['w8']\n",
    "w9 = params['w9']\n",
    "\n",
    "start = timeit.default_timer()\n",
    "print('Start submission generation')\n",
    "\n",
    "place_weights = {}\n",
    "for i in range(npt):\n",
    "    place_weights[i] = ( w0 / (i + w0) )\n",
    "\n",
    "sub_weight = [w1, w2, w3, w4, w5, w6, w7, w8, w9]\n",
    "w_sum = w1 + w2 + w3 + w4 + w5 + w6 + w7 + w8 + w9\n",
    "    \n",
    "print(f'W0: {w0}; Weight Percentage: {[i/w_sum*100 for i in sub_weight]}')\n",
    "\n",
    "result_final = {}\n",
    "result_final['Class'] = []\n",
    "result_final['Segments'] = []\n",
    "for p, row in enumerate(sub[0]):\n",
    "    target_weight = {}\n",
    "    for s in range(lg):\n",
    "        row1 = sub[s][p]\n",
    "        for ind, trgt in enumerate(row1[Htarget].split(' ')):\n",
    "            target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])\n",
    "    tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]\n",
    "    result_final['Class'].append(row1[Hlabel])\n",
    "    result_final['Segments'].append(\" \".join(tops_trgt))\n",
    "\n",
    "result_df_final = pd.DataFrame.from_dict(result_final)\n",
    "result_df_final['Class'] = result_df_final['Class'].astype(int)\n",
    "print(result_df_final.shape)\n",
    "print(result_df_final.head())\n",
    "\n",
    "print(f'Finished submission generation! Spent {timeit.default_timer() - start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_final.to_csv('G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/blend/blend1_8_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local and Cloud merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_files = [\n",
    "    'G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/blend\\\\MixNeXtVladModel_finetune_iter300_new.csv',\n",
    "    'G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/blend\\\\mixnextvlad_iter60_shift2.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/blend\\MixNeXtVladModel_finetune_iter300_new.csv\n",
      "(1000, 2)\n",
      "   Class                                           Segments\n",
      "0      3  d5yW:280 LEa7:135 4LI3:15 d5yW:80 LEa7:75 d5yW...\n",
      "1      7  xot7:80 pnAP:155 pnAP:150 sgFt:275 HhUk:170 LL...\n",
      "2      8  GLc8:15 tN2Z:35 KG7g:255 Tefx:165 SDNb:165 iOe...\n",
      "3     11  RBoT:110 zdov:165 RBoT:120 zdov:95 RBoT:100 RB...\n",
      "4     12  Qr1y:175 VzcL:45 yr73:140 0leM:245 JJ9a:155 0l...\n",
      "G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/blend\\mixnextvlad_iter60_shift2.csv\n",
      "(1000, 2)\n",
      "   Class                                           Segments\n",
      "0      3  LEa7:55 2tpp:20 d5yW:280 d5yW:75 F4ZL:185 mUYZ...\n",
      "1      7  i74x:5 i74x:0 rQKZ:60 xUFD:125 uf3s:150 NCE6:1...\n",
      "2      8  tN2Z:70 icIs:165 tN2Z:255 a5cE:155 tN2Z:150 Cr...\n",
      "3     11  hdTT:100 gF7h:115 0Ycu:100 nutU:270 hdTT:95 rZ...\n",
      "4     12  L7U7:30 uX9U:35 FXxf:90 yr73:230 yr73:280 L7U7...\n"
     ]
    }
   ],
   "source": [
    "#'''\n",
    "SUBDIR_SINGLE_MODEL = 'G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/blend/'\n",
    "\n",
    "for i, file in enumerate( sub_files ):\n",
    "    #file = 'G:/Competition/Kaggle-Youtube8M/submissions/local_CV/oof/sorted/v2\\\\NetVLADModelLF_hs2048_cs16_predictions_finetune_terence_oof.csv'\n",
    "    print(file)\n",
    "    sample_sub = pd.read_csv('../../../submissions/sample_submission.csv')\n",
    "    column_names = sample_sub.columns\n",
    "    sample_sub.set_index('Class', inplace=True)\n",
    "    \n",
    "    #pred = pd.read_csv(file, names=column_names)\n",
    "    pred = pd.read_csv(file)\n",
    "    print(pred.shape)\n",
    "    print(pred.head())\n",
    "    pred.set_index('Class', inplace=True)\n",
    "    for cls in sample_sub.index:\n",
    "        if cls in pred.index:\n",
    "            sample_sub.loc[cls, 'Segments'] = pred.loc[cls, 'Segments']\n",
    "    sample_sub.reset_index().to_csv(SUBDIR_SINGLE_MODEL+'sorted/'+file.split('\\\\')[-1].split('.')[0]+'_sorted.csv', index=None)\n",
    "    #break\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_files = [\n",
    "    'G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/paper/back\\\\MixNeXtVladModel_finetune_iter300_new.csv',\n",
    "    'G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/paper/blend\\\\blend1-6_final.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/paper/back\\MixNeXtVladModel_finetune_iter300_new.csv\n",
      "Reading 1: G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/paper/blend\\blend1-6_final.csv\n"
     ]
    }
   ],
   "source": [
    "Hlabel = 'Class' \n",
    "Htarget = 'Segments'\n",
    "npt = 100000 # number of places in target\n",
    "\n",
    "lg = len(sub_files)\n",
    "sub = [None]*lg\n",
    "for i, file in enumerate( sub_files ):\n",
    "    ## input files ##\n",
    "    print(\"Reading {}: {}\". format(i, file))\n",
    "    reader = csv.DictReader(open(file,\"r\"))\n",
    "    sub[i] = sorted(reader, key=lambda d: str(d[Hlabel]))\n",
    "    #if i > 1:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = 3.72384689385059\n",
    "w1_merge = 1\n",
    "w2_merge = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start submission generation\n",
      "W0: 3.72384689385059; Weight Percentage: [1, 9]\n",
      "(1000, 2)\n",
      "   Class                                           Segments\n",
      "0    100  AmKH:155 iAON:285 yeEH:275 ZvNy:140 yeEH:150 b...\n",
      "1   1000  pyZt:170 pyZt:220 pyZt:180 pyZt:175 pyZt:210 p...\n",
      "2   1001  1bhQ:45 1bhQ:80 1bhQ:145 TLOW:150 TLOW:235 1bh...\n",
      "3   1002  rtuW:125 rtuW:130 rtuW:170 rtuW:120 rtuW:155 r...\n",
      "4   1005  lSZ6:5 lSZ6:0 37Nx:140 37Nx:135 lSZ6:285 e2tL:...\n",
      "Finished submission generation! Spent 209.96366780000017s\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "print('Start submission generation')\n",
    "\n",
    "place_weights = {}\n",
    "for i in range(npt):\n",
    "    place_weights[i] = ( w0 / (i + w0) )\n",
    "\n",
    "sub_weight = [w1_merge, w2_merge]\n",
    "w_sum = w1_merge + w2_merge\n",
    "print(f'W0: {w0}; Weight Percentage: {sub_weight}')\n",
    "\n",
    "merge_final = {}\n",
    "merge_final['Class'] = []\n",
    "merge_final['Segments'] = []\n",
    "for p, row in enumerate(sub[0]):\n",
    "    target_weight = {}\n",
    "    for s in range(lg):\n",
    "        row1 = sub[s][p]\n",
    "        for ind, trgt in enumerate(row1[Htarget].split(' ')):\n",
    "            target_weight[trgt] = target_weight.get(trgt,0) + (place_weights[ind]*sub_weight[s])\n",
    "    tops_trgt = sorted(target_weight, key=target_weight.get, reverse=True)[:npt]\n",
    "    merge_final['Class'].append(row1[Hlabel])\n",
    "    merge_final['Segments'].append(\" \".join(tops_trgt))\n",
    "\n",
    "merge_df_final = pd.DataFrame.from_dict(merge_final)\n",
    "merge_df_final['Class'] = merge_df_final['Class'].astype(int)\n",
    "print(merge_df_final.shape)\n",
    "print(merge_df_final.head())\n",
    "\n",
    "print(f'Finished submission generation! Spent {timeit.default_timer() - start}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df_final.to_csv(f'G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/final/sorted/sorted_single/paper/blend/blend1-7.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>ari1:160 AmKH:155 lIpW:265 ZvNy:140 iAON:285 b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>pyZt:220 pyZt:175 pyZt:210 pyZt:180 pyZt:170 J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>1bhQ:45 1bhQ:150 TLOW:200 TLOW:225 1bhQ:80 TLO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>rtuW:125 rtuW:130 rtuW:155 rtuW:145 rtuW:140 r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>37Nx:140 lSZ6:5 37Nx:135 pRH7:75 eDaV:110 lSZ6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                           Segments\n",
       "0    100  ari1:160 AmKH:155 lIpW:265 ZvNy:140 iAON:285 b...\n",
       "1   1000  pyZt:220 pyZt:175 pyZt:210 pyZt:180 pyZt:170 J...\n",
       "2   1001  1bhQ:45 1bhQ:150 TLOW:200 TLOW:225 1bhQ:80 TLO...\n",
       "3   1002  rtuW:125 rtuW:130 rtuW:155 rtuW:145 rtuW:140 r...\n",
       "4   1005  37Nx:140 lSZ6:5 37Nx:135 pRH7:75 eDaV:110 lSZ6..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = pd.read_csv('G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/sorted/ensemble/ensemble_w1-w15.csv')\n",
    "ttt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>LdsT:265 ZvNy:140 bCnN:75 AmKH:155 LdsT:275 ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>pyZt:220 pyZt:180 pyZt:210 pyZt:170 pyZt:80 lK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>TLOW:200 TLOW:235 1bhQ:45 eCpP:0 TLOW:215 TLOW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002</td>\n",
       "      <td>rtuW:125 XrI5:20 3F8V:50 rtuW:155 rtuW:130 rtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>eDaV:110 LAIT:25 37Nx:135 37Nx:140 e2tL:190 lS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                           Segments\n",
       "0    100  LdsT:265 ZvNy:140 bCnN:75 AmKH:155 LdsT:275 ar...\n",
       "1   1000  pyZt:220 pyZt:180 pyZt:210 pyZt:170 pyZt:80 lK...\n",
       "2   1001  TLOW:200 TLOW:235 1bhQ:45 eCpP:0 TLOW:215 TLOW...\n",
       "3   1002  rtuW:125 XrI5:20 3F8V:50 rtuW:155 rtuW:130 rtu...\n",
       "4   1005  eDaV:110 LAIT:25 37Nx:135 37Nx:140 e2tL:190 lS..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt = pd.read_csv('G:/Competition/Kaggle-Youtube8M/submissions/local_CV/submission/sorted/ensemble/ensemble_w1-w8.csv')\n",
    "ttt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
